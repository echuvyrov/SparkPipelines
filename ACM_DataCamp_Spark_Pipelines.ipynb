{"cells":[{"cell_type":"code","source":["# Basic setup and acquisition of data\nfrom pyspark import SparkContext\nfrom pyspark.sql import Row, SQLContext, DataFrameNaFunctions\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\nfrom pyspark.ml.classification import RandomForestClassifier, LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n\nsqlCtx = SQLContext(sc)\ntitanic_train = sqlCtx.read.format('com.databricks.spark.csv').options(header='true', inferSchema='false').load('/mnt/s3_data/train.csv')\ntitanic_test = sqlCtx.read.format('com.databricks.spark.csv').options(header='true', inferSchema='false').load('/mnt/s3_data/test.csv')\n\n#split training data\n(df_train, df_test) = titanic_train.randomSplit([0.8, 0.2])\n\nprint df_train.take(1)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#setting up pipeline - get the StringType error\n# Configure an ML pipeline, which consists of these stages: VectorAssmembler and Indexer\nassembler = VectorAssembler(inputCols=[\"Pclass\", \"Gender\"], outputCol=\"features\")\nindexer = StringIndexer(inputCol=\"Survived\", outputCol=\"label\")\n\nrf = RandomForestClassifier(numTrees=3, maxDepth=4)\n\npipeline = Pipeline(stages=[assembler, indexer, rf])\n# Fit the pipeline to training documents.\nmodel = pipeline.fit(df_train)\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#we need to create our own Transformer to perform proper feature extraction and casting\nfrom pyspark.ml.pipeline import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.sql.functions import mean, min, max\n\nclass PickFeatures(Transformer):\n\n    def __init__(self):\n        super(PickFeatures, self).__init__()\n        \n    def convertColumn(self, df, name, new_type):\n        df_1 = df.withColumnRenamed(name, \"swap\")\n        return df_1.withColumn(name, df_1.swap.cast(new_type)).drop(\"swap\")\n\n    def _transform(self, dataset):\n        dataset = self.convertColumn(dataset, \"Pclass\", \"int\")\n        dataset = dataset.replace(\"\", \"0.0\", \"Age\")\n        dataset = self.convertColumn(dataset, \"Age\", \"float\")\n        \n        avgAge = dataset.select([mean('Age')]).first()[0]\n        dataset = dataset.replace(0, avgAge, \"Age\")        \n        dataset = self.convertColumn(dataset, \"Fare\", \"float\")\n        \n        return dataset"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#after creating our own Transformer, we are ready to try building the model again\nfeaturizer = PickFeatures()\nencoder = StringIndexer(inputCol=\"Sex\", outputCol=\"Gender\")\n\n#redefine assembler to include Age in features\nassembler = VectorAssembler(inputCols=[\"Pclass\", \"Age\", \"Fare\", \"Gender\"], outputCol=\"features\")\n\n#pipeline = Pipeline(stages=[featurizer, encoder, assembler, indexer, rf])\n\n#swap the algos\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline(stages=[featurizer, encoder, assembler, indexer, lr])\n\n# Fit the pipeline to training documents.\nmodel = pipeline.fit(df_train)\n\n# Make predictions on the held-out set of labeled data\nprediction = model.transform(df_test)\n# Evaluate the performance of the model\nevaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='precision')\n\n# Evaluate the predictions done on the test set\naccuracy = evaluator.evaluate(prediction)\nprint \"Accuracy = \" + str(accuracy)\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#make prediction on the new (unlabeled) data\n#this is the killer line that makes all the previous steps worthwhile\ntest_prediction = model.transform(titanic_test)\nselected = test_prediction.select(\"PassengerId\", \"prediction\")\nfor row in selected.collect():\n    print row\n\n#create a submission .csv file\n#selected.repartition(1).write.format('com.databricks.spark.csv').save('/mnt/s3_data/prediction7.csv')\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Improvements to our initial model\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n#1. Grid search\nparamGrid = ParamGridBuilder().addGrid(rf.numTrees, (3, 4, 5, 6))\\\n                              .addGrid(rf.maxDepth, (2, 3, 4, 5, 6))\\\n                              .build()\n\n#2. Model selection via cross-validation\n# Tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately\ncv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=MulticlassClassificationEvaluator(), numFolds=4)\n\nmodel = cv.fit(df_train)\n#make predictions using new model on our holdout set\nprediction = model.transform(df_test)\n\n# Evaluate the performance of the model\nevaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='precision')\n# Evaluate the predictions done on the test set\naccuracy = evaluator.evaluate(prediction)\nprint \"Accuracy = \" + str(accuracy)\n\n# Note that we could also use a TrainValidationSplit, which is less computationally expensive but also will not produce \n#  as reliable results when the training dataset is not sufficiently large.\ntv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=MulticlassClassificationEvaluator(), numFolds=4)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"Titanic_1","notebookId":3477},"nbformat":4,"nbformat_minor":0}
